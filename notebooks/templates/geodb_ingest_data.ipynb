{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bed6139-5f56-45c1-b01a-5be3dc00e34b",
   "metadata": {},
   "source": [
    "# Ingesting a new dataset into the geoDB from a local vector data file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c846327-a618-405b-a886-d2f9dc04cc3b",
   "metadata": {},
   "source": [
    "This notebook provides a template for ingesting new data from a local file into a geoDB collection, demonstrating some common techniques and operations which may be needed to prepare data, create a collection, and insert or update database rows. The notebook is not ready to run as is, and must be adapted for the relevant task by adding credentials, the path to the input data, the database name, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc08553-2b08-48b4-b4aa-a58dd5567100",
   "metadata": {},
   "source": [
    "## Initial set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a3de6f-dd21-4d90-873e-9255355ff8b2",
   "metadata": {},
   "source": [
    "Some standard imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d73614-20d1-49c6-85f6-2a12ff09b133",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas\n",
    "from xcube_geodb.core.geodb import GeoDBClient\n",
    "import geopandas\n",
    "from pyproj.crs import CRS\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124c95b6-08bb-4f47-89bb-87b9e52d173c",
   "metadata": {},
   "source": [
    "Import pyplot and set a sensible figure size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23136409-736f-4c41-a0bf-7ea9a284cf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = 16,12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464cfd82-fe22-471b-bafc-49b33891bebb",
   "metadata": {},
   "source": [
    "Set geoDB credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f5f475-0ebb-4a90-97b9-9db79d4ea6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GEODB_AUTH_CLIENT_ID'] = '[CLIENT ID GOES HERE]'\n",
    "os.environ['GEODB_AUTH_CLIENT_SECRET'] = '[CLIENT SECRET GOES HERE]'\n",
    "os.environ['GEODB_AUTH_MODE'] = 'client-credentials'\n",
    "os.environ['GEODB_AUTH_AUD'] = 'https://geodb.brockmann-consult.de'\n",
    "os.environ['GEODB_API_SERVER_URL'] = 'https://xcube-geodb.brockmann-consult.de'\n",
    "os.environ['GEODB_AUTH_DOMAIN'] = 'https://xcube-users.brockmann-consult.de/api/v2'\n",
    "os.environ['GEOSERVER_SERVER_URL'] = 'https://xcube-geodb.brockmann-consult.de'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b07e2c-3299-4094-a01c-24bedaf9f4bf",
   "metadata": {},
   "source": [
    "## Read and examine the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3541f7fd-f9f5-478e-ba35-0299d7f35fd3",
   "metadata": {},
   "source": [
    "Open the data file as a GeoDataFrame. GeoPandas can read (amongst others) shapefiles, zipped shapefile directories, and gpkgs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba3fa8d-3ead-42f2-8e8d-e4380e2ac5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = geopandas.read_file(\"[INPUT FILENAME GOES HERE].zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58978a3b-4e5d-47ca-aab6-dab8d2a78503",
   "metadata": {},
   "source": [
    "Check that the data looks sane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c30f54-7292-43c0-b809-8f05c3dc90ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50092b89-8d80-4bfe-8991-26a4a9d8e939",
   "metadata": {},
   "source": [
    "Plot the data as a further sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a10454-dcbb-4e64-87d5-86dbe3919135",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a30870-d516-496b-ba5b-e4fd8bad0088",
   "metadata": {},
   "source": [
    "## Make the data suitable for ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9cc720-12a1-477c-aa0d-0377bf65589c",
   "metadata": {},
   "source": [
    "First, check the WKT for the projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bbc166-ccde-4b4f-a00a-4a6869f68794",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.crs.to_wkt()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919e4bca-9eea-44d3-908c-a827a6e48520",
   "metadata": {},
   "source": [
    "This could be a non-EPSG CRS. Let's make sure by asking GeoPandas for its EPSG code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62d1f88-38a1-46d3-ab82-82154825f999",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.crs.to_epsg() is None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477e68f4-956a-4f3b-ac9d-b7991e2ee5b4",
   "metadata": {},
   "source": [
    "Currently GeoPandas doesn't support non-EPSG CRSs, so if this isn't EPSG we'll have to reproject before ingesting it. The target CRS must be chosen according to the are and application. For demonstration purposes, we use EPSG 31370 below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7c4257-a996-4a0b-a21f-c896907190fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_31370 = df.to_crs(epsg=31370)\n",
    "df_31370"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b017f3-66b0-4b93-b2b3-42a567afd796",
   "metadata": {},
   "source": [
    "Let's just make sure that the CRS is now as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e668def6-ce97-4381-9e50-0847e4a84e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_31370.crs.to_epsg()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93fef7b-6a3b-4c25-913b-5df1a3e89449",
   "metadata": {},
   "source": [
    "Now we need to check the types of the columns in the GeoDataFrame, so we can create corresponding columns in the geoDB collection. We're doing this before `NaN` replacement, because `NaN` replacement can mess with the dtypes â€“ see e.g. <https://stackoverflow.com/questions/59500812/pandas-dataframe-replace-change-dtype-of-columns>. We can use the `dtypes` property to see the types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcb57d8-1d7f-47ec-b0f5-02fd46936ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_31370.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0950f00d-0324-41c1-b19e-bf2a66865eaf",
   "metadata": {},
   "source": [
    "The `OBJECTID` has type `float64`, which isn't ideal: we know the ID is an integer, and can see it in the quick view of the DataFrame above. Let's cast it to an `int64` before putting it in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e91889-d296-4864-ba1b-e064f3cfd137",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_31370[\"OBJECTID\"] = df_31370[\"OBJECTID\"].astype(\"int64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f61916b-f362-43fe-b5ec-cc0a96d60691",
   "metadata": {},
   "source": [
    "Now check if there are any `NaN` values in the DataFrame. If so, we'll need to replace them with `None` before ingestion. This check is mainly for curiosity: we can do the replace in any case, and it will be a no-op if there aren't any `NaN`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9eea4c-63b0-486c-b620-d835b0e32d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_counts = (df_31370.eq(np.NaN)).sum()\n",
    "nan_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603bfbfb-028d-4e08-bb13-8d060a184fae",
   "metadata": {},
   "source": [
    "If we do have any `NaN`s, replace them with `None`s. We only do this if there *are* `NaN`s, since it can alter the `dtype`s (see above). This is a fairly crude solution: it would probably be better to do the replace on a per-column basis and only for columns which do contain `NaN`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a59288e-0201-47e5-9539-ec19009bf9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "if nan_counts.sum() > 0:\n",
    "    df_to_ingest = df_31370.replace({np.NaN: None})\n",
    "else:\n",
    "    df_to_ingest = df_31370"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d27db2-12e0-4aa9-932b-cf07c4a3a8b4",
   "metadata": {},
   "source": [
    "## Ingest the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f56d92e-7d74-4aeb-a0b2-bd0272723534",
   "metadata": {},
   "source": [
    "Now we need to add our data as a geoDB collection. First, connect to the geoDB and check our user ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b55420-f396-42bd-bf33-761349dc2b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "geodb = GeoDBClient()\n",
    "geodb.whoami"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfec6f84-78d3-4e71-a85a-2ce6ec190dbe",
   "metadata": {},
   "source": [
    "Make sure that our desired database is available as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d625ce0f-e002-418d-9f32-b2d101bb7412",
   "metadata": {},
   "outputs": [],
   "source": [
    "geodb.get_my_databases()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973d28b8-9561-4d83-8dc7-574f9a8201a8",
   "metadata": {},
   "source": [
    "Now let's see what collections we currently have in the database, so we can pick an appropriate name for the new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8a37a7-e3d5-49a7-8c83-75716f0d2a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_database_name = \"my_database\"\n",
    "geodb.get_my_collections(database=target_database_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cec566-4825-4617-8e89-bb7a3b790090",
   "metadata": {},
   "source": [
    "Set a variable for the collection name, used both to create the collection (if required) and insert the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7593afa5-004e-4efc-82d8-97a1c2dc828d",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"my_collection_name\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d581652-32a2-4fe7-af66-787d9a8e961c",
   "metadata": {},
   "source": [
    "Now we need to create a new collection for the data. The database columns are supplied in the dictionary given as the `properties` parameter to `create_collection_if_not_exists`. Note that we don't explicitly specify a `geometry` column: this is included automatically, since it must always be present (otherwise it wouldn't be a geodatabaseâ€¦). The `int64` and `float64` dtypes map to `int` and `float` in the properties lists, and all the `object` dtypes actually represent strings, so we use the `text` type for them in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542f1f00-23d2-45cc-b4ef-e70c74a3ac1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = geodb.create_collection_if_not_exists(\n",
    "    collection=collection_name,\n",
    "    crs=df_to_ingest.crs.to_epsg(),\n",
    "    properties={\n",
    "        \"OBJECTID\": \"int\",\n",
    "        \"CAMPAGNE\": \"text\",\n",
    "        \"CULT_COD\": \"text\",\n",
    "        \"CULT_NOM\": \"text\",\n",
    "        \"GROUPE_CUL\": \"text\",\n",
    "        \"SURF_HA\":  \"float\",\n",
    "    },\n",
    "    database=target_database_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f06486-7929-49fb-a7be-361738f41289",
   "metadata": {},
   "source": [
    "Finally, we insert the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2db48d-fad9-4e78-a5f9-6c3d96346798",
   "metadata": {},
   "outputs": [],
   "source": [
    "geodb.insert_into_collection(collection=collection_name, values=df_to_ingest, database=target_database_name, upsert=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa676dd-7211-4acd-abfd-2f2fa0e59ed4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
